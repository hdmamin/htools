{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T23:23:10.921219Z",
     "start_time": "2021-05-12T23:23:08.497179Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tldextract import extract\n",
    "\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T23:23:12.600715Z",
     "start_time": "2021-05-12T23:23:10.923437Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path(\n",
    "    '~/DatascienceBase/Delphi/v3-0-0/data/alpha_test/beacon_urls_851105.csv'\n",
    ").expanduser()\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T23:54:42.740914Z",
     "start_time": "2021-05-12T23:54:41.668715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonmamin/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11111"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqdns = [f for f in sorted(df.fqdn.str.replace('http://', '')\n",
    "                                  .str.replace('www\\d?\\.', '').unique()) \n",
    "         if f]\n",
    "len(fqdns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T23:55:08.975311Z",
     "start_time": "2021-05-12T23:55:08.942726Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_fqdn(fqdn):\n",
    "    suff = extract(fqdn).suffix\n",
    "    return fqdn.rpartition(suff)[0]\\\n",
    "               .replace('.', ' ')\\\n",
    "               .strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T23:55:09.802942Z",
     "start_time": "2021-05-12T23:55:09.683218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11111, 10978)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2proc = {fqdn: preprocess_fqdn(fqdn) for fqdn in fqdns}\n",
    "proc2raw = {v: k for k, v in raw2proc.items()}\n",
    "\n",
    "# Some domains resolve to the same preprocessed domain. Handle later.\n",
    "len(raw2proc), len(proc2raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T23:55:32.654648Z",
     "start_time": "2021-05-12T23:55:32.622528Z"
    }
   },
   "outputs": [],
   "source": [
    "fd = FuzzyKeyDict(proc2raw, scorer=fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:14:24.554637Z",
     "start_time": "2021-05-13T00:14:24.510338Z"
    }
   },
   "outputs": [],
   "source": [
    "def nearest_match(query, threshold=75, fuzzy=fd):\n",
    "    if query in fuzzy.values():\n",
    "        print('Found exact match.')\n",
    "        return query\n",
    "    if ' ' in query or '.' not in query:\n",
    "        print('Non URL. Looking for fuzzy match with token_sort_ratio.')\n",
    "        proc, match, score = fuzzy.similar(query,\n",
    "                                           mode='keys_values_similarities')[0]\n",
    "    else:\n",
    "        print('Non-present URL. Looking for fuzzy match with default ratio.')\n",
    "        query = re.sub('www\\d?\\.', '', query.replace('http://', ''))\n",
    "        match, score = process.extractOne(query, fuzzy.values())\n",
    "    print('score:', score)\n",
    "    return match if score >= threshold else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:14:30.189003Z",
     "start_time": "2021-05-13T00:14:26.581318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: classroom.google.com\n",
      "Found exact match.\n",
      "classroom.google.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: www.classroom.google.com\n",
      "Non-present URL. Looking for fuzzy match with default ratio.\n",
      "score: 100\n",
      "classroom.google.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: www.espn.com\n",
      "Non-present URL. Looking for fuzzy match with default ratio.\n",
      "score: 100\n",
      "espn.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: espn.go.com\n",
      "Non-present URL. Looking for fuzzy match with default ratio.\n",
      "score: 95\n",
      "espn.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: google classroom\n",
      "Non URL. Looking for fuzzy match with token_sort_ratio.\n",
      "score: 100\n",
      "classroom.google.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: google meet\n",
      "Non URL. Looking for fuzzy match with token_sort_ratio.\n",
      "score: 100\n",
      "meet.google.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: google docs\n",
      "Non URL. Looking for fuzzy match with token_sort_ratio.\n",
      "score: 100\n",
      "docs.google.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: clever\n",
      "Non URL. Looking for fuzzy match with token_sort_ratio.\n",
      "score: 100\n",
      "clever.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: youtube\n",
      "Non URL. Looking for fuzzy match with token_sort_ratio.\n",
      "score: 100\n",
      "youtube.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: khanacademy.com\n",
      "Non-present URL. Looking for fuzzy match with default ratio.\n",
      "score: 95\n",
      "khanacademy.fandom.com\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: kahnacademy.org\n",
      "Non-present URL. Looking for fuzzy match with default ratio.\n",
      "score: 93\n",
      "khanacademy.org\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "query: kahnacademy\n",
      "Non URL. Looking for fuzzy match with token_sort_ratio.\n",
      "score: 91\n",
      "khanacademy.org\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    'classroom.google.com',\n",
    "    'www.classroom.google.com',\n",
    "    'www.espn.com',\n",
    "    'espn.go.com',\n",
    "    'google classroom',\n",
    "    'google meet', \n",
    "    'google docs',\n",
    "    'clever',\n",
    "    'youtube',\n",
    "    'khanacademy.com',   # Wrong suffix\n",
    "    'kahnacademy.org',   # Typo\n",
    "    'kahnacademy'        # Typo and no suffix\n",
    "]\n",
    "for query in queries:\n",
    "    print('query:', query)\n",
    "    print(nearest_match(query))\n",
    "    print(spacer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend LSHDict\n",
    "\n",
    "See if I can tweak lshdict to allow us to work on lists of strings rather than strings.\n",
    "\n",
    "UPDATE: Seems like this would require changes sufficiently large to justify a separate implementation (if I decide it's worth it at some point). Don't try to shoehorn it into this implementation: think of this as StrLSHDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:20:28.459773Z",
     "start_time": "2021-05-13T00:20:28.429368Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from htools.structures import _FuzzyDictBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:29:49.772385Z",
     "start_time": "2021-05-13T00:29:49.729962Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSHDict(_FuzzyDictBase):\n",
    "    \"\"\"Dictionary that returns the value corresponding to a key's nearest\n",
    "    neighbor if the key isn't present in the dict. This is intended for use\n",
    "    as a word2index dict when using embeddings in deep learning: e.g. if we\n",
    "    have domain embeddings for the top 100k websites, some of our options for\n",
    "    dealing with unknown domains are:\n",
    "\n",
    "    1. Encode all of them as <UNK>. This loses a lot of information.\n",
    "    2. Create a FuzzyKeyDict which will search for similar keys using variants\n",
    "    of Levenshtein distance. Lookup is O(N) and for 100k domains, that comes\n",
    "    out to 0.6 seconds per item. We might have thousands or millions of\n",
    "    lookups over the course of training so this can be a significant cost.\n",
    "    3. Create an LSHDict (lookups are O(1)). Indexing into the dict as usual\n",
    "    (e.g. my_lsh_dict[key]) will provide the key's index if present and the\n",
    "    (approximate) nearest neighbor's index otherwise. Either way, the result\n",
    "    can be used to index into your embedding layer.\n",
    "    4. Create an LSHDict and use the `similar_values` method to return n>1\n",
    "    neighbors. Then pass their indices to an Embedding layer and\n",
    "    compute the sum/average/weighted average of the results. This may be\n",
    "    preferable to #3 cases such as web domain lookup, where similar URLs are\n",
    "    not guaranteed to represent similar sites. (This is basically\n",
    "    equivalent to an EmbeddingBag layer, but in torch that doesn't store\n",
    "    intermediate representations so we wouldn't be able to use our pretrained\n",
    "    embeddings.)\n",
    "\n",
    "    LSHDict does NOT support pickling as of version 6.0.6 (note: setitem seems\n",
    "    to be called before init when unpickling, meaning we try to access\n",
    "    self.forest in self._update_forest before it's been defined. Even if we\n",
    "    change setitem so reindexing does not occur by default, it still tries to\n",
    "    hash the new word and add it to the forest so unpickling will still fail).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, n_candidates=None, n_keys=3, ngram_size=3,\n",
    "                 scorer=fuzz.ratio):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: dict or list[tuple]\n",
    "            The base dictionary. Unlike FuzzyKeyDict, we require this since\n",
    "            adding items one by one is computationally infeasible for large\n",
    "            datasets. Just build up your dictionary first.\n",
    "        n_candidates: int or None\n",
    "            Number of reasonably similar keys to retrieve when trying to index\n",
    "            in with a key that's missing (or when using the `similar` method).\n",
    "            You can override this in `similar` but not when using\n",
    "            __getitem__'s square bracket syntax. If not specified, this will\n",
    "            be auto initialized to vocab size/1,000, clipped to lie in\n",
    "            [20, 500]. See `similar` docstring for more on this.\n",
    "        n_keys: int\n",
    "            Default number of similar keys to retrieve in `similar`.\n",
    "        scorer: function\n",
    "            Default scoring function to use to narrow `n_candidates` keys down\n",
    "            to `n_keys`. Should be a fuzzywuzzy function where scores lie in\n",
    "            [0, 100] and higher values indicate high similarity.\n",
    "        \"\"\"\n",
    "        if len(data) < 10_000 and len(next(iter(data))) < 100:\n",
    "            warnings.warn(\n",
    "                'It looks like you\\'re working with a relatively small '\n",
    "                'amount of data. FuzzyKeyDict may be fast enough for your '\n",
    "                'use case and would provide the set of strictly most similar '\n",
    "                'keys rather than an approximation of that set.'\n",
    "            )\n",
    "\n",
    "        super().__init__(data)\n",
    "        self.scorer = scorer\n",
    "        self.hash_word = partial(self.lsh_hash_word, n=ngram_size)\n",
    "        self.forest = MinHashLSHForest(num_perm=128)\n",
    "        self._initialize_forest()\n",
    "\n",
    "        # Datasketch's LSH implementation usually gives pretty decent results\n",
    "        # even with numbers as low as 5-10, but increasing that by a factor of\n",
    "        # 10 comes with minimal time cost: Fuzzywuzzy matching doesn't get\n",
    "        # particularly slow until we get into the thousands. The fact that\n",
    "        # we cap this at 500 makes this lookup asymptotically O(1) while\n",
    "        # FuzzyKeyDict's is O(N).\n",
    "        self.n_candidates = n_candidates or np.clip(len(self) // 1_000,\n",
    "                                                    20, 500)\n",
    "        self.n_keys = n_keys\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        \"\"\"Try to add keys all at once in the constructor because adding new\n",
    "        keys can be extremely slow.\n",
    "        \"\"\"\n",
    "        super().__setitem__(key, val)\n",
    "        self._update_forest(key, val)\n",
    "\n",
    "    def _update_forest(self, key, val, index=True):\n",
    "        \"\"\"Used in __setitem__ to update our LSH Forest. Forest's index method\n",
    "        seems to recompute everything so adding items to a large LSHDict will\n",
    "        be incredibly slow. Luckily, our deep learning use case rarely/never\n",
    "        requires us to update object2index dicts after instantiation so that's\n",
    "        not as troubling as it might seem.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key: str\n",
    "        val: any\n",
    "        index: bool\n",
    "            If True, reindex the forest (essentially making the key\n",
    "            queryable). This should be False when initializing the forest so\n",
    "            we just index once after everything's been added.\n",
    "        \"\"\"\n",
    "        self.forest.add(key, self.hash_word(key))\n",
    "        if index: self.forest.index()\n",
    "\n",
    "    def _initialize_forest(self):\n",
    "        \"\"\"Called once in __init__ to add all items to LSH Forest. This is\n",
    "        necessary because dict specifically calls its own __setitem__, not\n",
    "        its children's.\n",
    "        \"\"\"\n",
    "        for k, v in self.items():\n",
    "            self._update_forest(k, v, False)\n",
    "        self.forest.index()\n",
    "\n",
    "    @add_docstring(_FuzzyDictBase._filter_similarity_pairs)\n",
    "    def similar(self, key, mode='keys_values', n_candidates=None,\n",
    "                n_keys=None, scorer=None):\n",
    "        \"\"\"Find a list of similar keys. This is used in __getitem__ but can\n",
    "        also be useful as a user-facing method if you want to get more than\n",
    "        1 neighbor or you want to get similarity scores as well.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key: str\n",
    "            Word/URL/etc. to find similar keys to.\n",
    "        mode: str\n",
    "            See section below `Returns`.\n",
    "        n_candidates: int or None\n",
    "            Number of similar candidates to retrieve. This uses Jaccard\n",
    "            Similarity which isn't always a great metric for string\n",
    "            similarity. This is also where the LSH comes in so they're not\n",
    "            strictly the n best candidates, but rather a close approximation\n",
    "            of that set. If None, this will fall back to self.n_candidates.\n",
    "            Keep in mind this determines how many keys to\n",
    "        n_keys: int or None\n",
    "            Number of similar keys to return. If None, this will fall back to\n",
    "            self.n_keys.\n",
    "        scorer: function or None\n",
    "            Fuzzywuzzy scoring function, e.g. fuzz.ratio or\n",
    "            fuzz.partial_ratio, which will be used to score each candidate and\n",
    "            select which to return. Higher scores indicate higher levels of\n",
    "            similarity. If None, this will fall back to self.scorer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list: List if `mode` is \"keys\" or \"values\". List of tuples otherwise.\n",
    "        \"\"\"\n",
    "        candidates = self.forest.query(self.hash_word(key),\n",
    "                                       n_candidates or self.n_candidates)\n",
    "        if not candidates: raise KeyError('No similar keys found.')\n",
    "\n",
    "        # List of (key, score) where higher means more similar.\n",
    "        pairs = process.extract(key, candidates,\n",
    "                                limit=n_keys or self.n_keys,\n",
    "                                scorer=scorer or self.scorer)\n",
    "        return self._filter_similarity_pairs(pairs, mode=mode)\n",
    "\n",
    "#     @staticmethod\n",
    "#     @add_docstring(ngrams)\n",
    "#     def lsh_hash_word(word, num_perm=128, **ngram_kwargs):\n",
    "#         \"\"\"Hash an input word (str) and return a MinHash object that can be\n",
    "#         added to an LSHForest.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         word: str\n",
    "#             Word to hash.\n",
    "#         num_perm: int\n",
    "#         ngram_kwargs: any\n",
    "#             Forwarded to `ngrams`.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         datasketch MinHash object\n",
    "#         \"\"\"\n",
    "#         mhash = MinHash(num_perm=num_perm)\n",
    "#         for ng in ngrams(word, **ngram_kwargs):\n",
    "#             # TODO: diff\n",
    "#             print(ng)\n",
    "#             mhash.update(ng.encode('utf8'))\n",
    "#         return mhash\n",
    "\n",
    "    @staticmethod\n",
    "    @add_docstring(ngrams)\n",
    "    def lsh_hash_word(word, num_perm=128, **ngram_kwargs):\n",
    "        \"\"\"Hash an input word (str) and return a MinHash object that can be\n",
    "        added to an LSHForest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word: str\n",
    "            Word to hash.\n",
    "        num_perm: int\n",
    "        ngram_kwargs: any\n",
    "            Forwarded to `ngrams`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        datasketch MinHash object\n",
    "        \"\"\"\n",
    "        mhash = MinHash(num_perm=num_perm)\n",
    "        for ng in ngrams(word, **ngram_kwargs):\n",
    "            # TODO: diff\n",
    "            if isinstance(ng, tuple) and len(ng) == 1: ng = ng[0]\n",
    "            mhash.update(ng.encode('utf8'))\n",
    "        return mhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:23:51.228222Z",
     "start_time": "2021-05-13T00:23:51.196820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('classroom', 'google'): 'classroom.google.com',\n",
       " ('khanacademy',): 'khanacademy.org',\n",
       " ('meet', 'google'): 'meet.google.com',\n",
       " ('google',): 'google.it',\n",
       " ('docs', 'google'): 'docs.google.com'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks2raw = {\n",
    "    tuple(k.split()): v for k, v in \n",
    "    select(proc2raw, ['classroom google', 'khanacademy', 'meet google',\n",
    "                      'google', 'docs google']).items()\n",
    "}\n",
    "chunks2raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:26:42.855972Z",
     "start_time": "2021-05-13T00:26:42.822502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('classroom',), ('google',)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(list(chunks2raw)[0], n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:26:56.691440Z",
     "start_time": "2021-05-13T00:26:56.658562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('classroom', 'google')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(list(chunks2raw)[0], n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:27:31.930724Z",
     "start_time": "2021-05-13T00:27:31.901212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'bcd']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:24:51.975788Z",
     "start_time": "2021-05-13T00:24:51.942426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abc']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(['abc'], n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:32:18.373873Z",
     "start_time": "2021-05-13T00:32:18.338175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonmamin/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: It looks like you're working with a relatively small amount of data. FuzzyKeyDict may be fast enough for your use case and would provide the set of strictly most similar keys rather than an approximation of that set.\n"
     ]
    }
   ],
   "source": [
    "lsh = LSHDict(chunks2raw, ngram_size=1, n_candidates=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:32:19.580038Z",
     "start_time": "2021-05-13T00:32:19.547766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('classroom', 'google'): 'classroom.google.com',\n",
       " ('khanacademy',): 'khanacademy.org',\n",
       " ('meet', 'google'): 'meet.google.com',\n",
       " ('google',): 'google.it',\n",
       " ('docs', 'google'): 'docs.google.com'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:34:54.826881Z",
     "start_time": "2021-05-13T00:34:54.769926Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/htools/htools/structures.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('google', 'classroom')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-b95a359e58cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'google'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classroom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlsh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/htools/htools/structures.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init_subclass__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/htools/htools/meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-7dbe37cdbc59>\u001b[0m in \u001b[0;36msimilar\u001b[0;34m(self, key, mode, n_candidates, n_keys, scorer)\u001b[0m\n\u001b[1;32m    153\u001b[0m         pairs = process.extract(key, candidates,\n\u001b[1;32m    154\u001b[0m                                 \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_keys\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                                 scorer=scorer or self.scorer)\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_similarity_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/fuzzywuzzy/process.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(query, choices, processor, scorer, limit)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[1;32m    167\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractWithoutOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/heapq.py\u001b[0m in \u001b[0;36mnlargest\u001b[0;34m(n, iterable, key)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/fuzzywuzzy/process.py\u001b[0m in \u001b[0;36mextractWithoutOrder\u001b[0;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Run the processor on the input query.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mprocessed_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_query\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/fuzzywuzzy/utils.py\u001b[0m in \u001b[0;36mfull_process\u001b[0;34m(s, force_ascii)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masciidammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Keep only Letters and Numbers (see Unicode docs).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mstring_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_non_letters_non_numbers_with_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Force into lowercase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mstring_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lower_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/fuzzywuzzy/string_processing.py\u001b[0m in \u001b[0;36mreplace_non_letters_non_numbers_with_whitespace\u001b[0;34m(cls, a_string)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnumbers\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mwhite\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \"\"\"\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mstrip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "key = ('google', 'classroom')\n",
    "lsh[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:36:04.162847Z",
     "start_time": "2021-05-13T00:36:04.128620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('google', 'classroom')\n",
      "[('classroom', 'google')]\n"
     ]
    }
   ],
   "source": [
    "match = lsh.forest.query(lsh.hash_word(key), lsh.n_candidates)\n",
    "print(key)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T00:40:22.546313Z",
     "start_time": "2021-05-13T00:40:22.498966Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-51a1ebccd1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_similarity_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keys_values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/htools/htools/structures.py\u001b[0m in \u001b[0;36m_filter_similarity_pairs\u001b[0;34m(self, pairs, mode)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'keys_values'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'keys_similarities'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/htools/htools/structures.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'keys_values'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'keys_similarities'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/htools/htools/structures.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "lsh._filter_similarity_pairs(((match, -1),), mode='keys_values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
